use crate::entities::{
  AgentConfigPB, ChatMessageErrorPB, ChatMessageListPB, ChatMessagePB, PredefinedFormatPB,
  RepeatedRelatedQuestionPB, StreamMessageParams,
};
use crate::middleware::chat_service_mw::ChatServiceMiddleware;
use crate::notification::{ChatNotification, chat_notification_builder};
use tracing::info;
use crate::stream_message::{AIFollowUpData, StreamMessage};
use allo_isolate::Isolate;
use flowy_ai_pub::cloud::{
  AIModel, ChatCloudService, ChatMessage, MessageCursor, QuestionStreamValue, ResponseFormat,
};
use flowy_ai_pub::persistence::{
  ChatMessageTable, select_answer_where_match_reply_message_id, select_chat_messages,
  upsert_chat_messages,
};
use flowy_ai_pub::user_service::AIUserService;
use flowy_error::{ErrorCode, FlowyError, FlowyResult};
use flowy_sqlite::DBConnection;
use futures::{SinkExt, StreamExt};
use lib_infra::isolate_stream::IsolateSink;
use std::path::PathBuf;
use std::sync::Arc;
use std::sync::atomic::{AtomicBool, AtomicI64};
use tokio::sync::{Mutex, RwLock};
use tracing::{error, instrument, trace, warn};
use uuid::Uuid;

enum PrevMessageState {
  HasMore,
  NoMore,
  Loading,
}

pub struct Chat {
  chat_id: Uuid,
  uid: i64,
  user_service: Arc<dyn AIUserService>,
  chat_service: Arc<ChatServiceMiddleware>,
  prev_message_state: Arc<RwLock<PrevMessageState>>,
  latest_message_id: Arc<AtomicI64>,
  stop_stream: Arc<AtomicBool>,
  stream_buffer: Arc<Mutex<StringBuffer>>,
}

impl Chat {
  pub fn new(
    uid: i64,
    chat_id: Uuid,
    user_service: Arc<dyn AIUserService>,
    chat_service: Arc<ChatServiceMiddleware>,
  ) -> Chat {
    Chat {
      uid,
      chat_id,
      chat_service,
      user_service,
      prev_message_state: Arc::new(RwLock::new(PrevMessageState::HasMore)),
      latest_message_id: Default::default(),
      stop_stream: Arc::new(AtomicBool::new(false)),
      stream_buffer: Arc::new(Mutex::new(StringBuffer::default())),
    }
  }

  pub fn close(&self) {}

  pub async fn stop_stream_message(&self) {
    self
      .stop_stream
      .store(true, std::sync::atomic::Ordering::SeqCst);
  }

  #[instrument(level = "info", skip_all, err)]
  pub async fn stream_chat_message(
    &self,
    params: &StreamMessageParams,
    preferred_ai_model: AIModel,
    agent_config: Option<AgentConfigPB>,
    tool_call_handler: Option<Arc<crate::agent::ToolCallHandler>>,  // ğŸ”§ å·¥å…·è°ƒç”¨å¤„ç†å™¨
    custom_system_prompt: Option<String>,  // ğŸ†• è‡ªå®šä¹‰ç³»ç»Ÿæç¤º(å·²åŒ…å«å·¥å…·è¯¦æƒ…)
  ) -> Result<ChatMessagePB, FlowyError> {
    let agent_name = agent_config.as_ref().map(|c| c.name.as_str()).unwrap_or("None");
    trace!(
      "[Chat] stream chat message: chat_id={}, message={}, message_type={:?}, format={:?}, agent={}",
      self.chat_id, params.message, params.message_type, params.format, agent_name,
    );

    // clear
    self
      .stop_stream
      .store(false, std::sync::atomic::Ordering::SeqCst);
    self.stream_buffer.lock().await.clear();

    let mut question_sink = IsolateSink::new(Isolate::new(params.question_stream_port));
    let answer_stream_buffer = self.stream_buffer.clone();
    let uid = self.user_service.user_id()?;
    let workspace_id = self.user_service.workspace_id()?;

    // æ„å»ºå¢å¼ºçš„ç³»ç»Ÿæç¤ºè¯ï¼ˆå¦‚æœæœ‰æ™ºèƒ½ä½“é…ç½®ï¼‰
    let system_prompt = if let Some(custom_prompt) = custom_system_prompt {
      // ğŸ†• ä½¿ç”¨è‡ªå®šä¹‰æç¤º(å·²åŒ…å«å·¥å…·è¯¦æƒ…)
      info!("[Chat] ğŸ”§ Using custom system prompt (with tool details)");
      Some(custom_prompt)
    } else if let Some(ref config) = agent_config {
      use crate::agent::{build_agent_system_prompt, AgentCapabilityExecutor};
      
      // åˆ›å»ºèƒ½åŠ›æ‰§è¡Œå™¨
      let capability_executor = AgentCapabilityExecutor::new(self.user_service.clone());
      
      // åŠ è½½å¯¹è¯å†å²ï¼ˆå¦‚æœå¯ç”¨äº†è®°å¿†åŠŸèƒ½ï¼‰
      let conversation_history = capability_executor
        .load_conversation_history(&self.chat_id, &config.capabilities, uid)
        .unwrap_or_default();
      
      info!(
        "[Chat] Loaded {} messages from conversation history", 
        conversation_history.len()
      );
      
      // æ„å»ºåŸºç¡€ç³»ç»Ÿæç¤ºè¯
      let base_prompt = build_agent_system_prompt(config);
      
      // æ„å»ºå¢å¼ºçš„ç³»ç»Ÿæç¤ºè¯ï¼ˆåŒ…å«å†å²ã€å·¥å…·æŒ‡å—ç­‰ï¼‰
      let enhanced_prompt = capability_executor.build_enhanced_system_prompt(
        base_prompt,
        config,
        &conversation_history,
      );
      
      info!(
        "[Chat] Using agent '{}' with enhanced system prompt ({} chars)",
        config.name,
        enhanced_prompt.len()
      );
      
      // æ£€æŸ¥æ˜¯å¦éœ€è¦ä»»åŠ¡è§„åˆ’
      if capability_executor.should_create_plan(&config.capabilities, &params.message) {
        info!("[Chat] Complex task detected, task planning recommended");
        // TODO: é›†æˆä»»åŠ¡è§„åˆ’å™¨
      }
      
      // æ£€æŸ¥æ˜¯å¦éœ€è¦å·¥å…·è°ƒç”¨
      if capability_executor.should_use_tools(&config.capabilities, &params.message) {
        info!("[Chat] Tool usage recommended for this request");
        // TODO: å‡†å¤‡å·¥å…·è°ƒç”¨ä¸Šä¸‹æ–‡
      }
      
      Some(enhanced_prompt)
    } else {
      None
    };

    // ä¿å­˜åŸå§‹ç”¨æˆ·æ¶ˆæ¯åˆ°æ•°æ®åº“ï¼ˆä¸åŒ…å«ç³»ç»Ÿæç¤ºè¯ï¼‰
    let question = self
      .chat_service
      .create_question(
        &workspace_id,
        &self.chat_id,
        &params.message,  // ä½¿ç”¨åŸå§‹æ¶ˆæ¯
        params.message_type.clone(),
        params.prompt_id.clone(),
      )
      .await
      .map_err(|err| {
        error!("Failed to send question: {}", err);
        FlowyError::server_error()
      })?;

    let _ = question_sink
      .send(StreamMessage::MessageId(question.message_id).to_string())
      .await;

    // Save message to disk
    notify_message(&self.chat_id, question.clone())?;
    let format = params.format.clone().map(Into::into).unwrap_or_default();
    
    // ä¼ é€’ç³»ç»Ÿæç¤ºè¯ã€æ™ºèƒ½ä½“é…ç½®å’Œå·¥å…·è°ƒç”¨å¤„ç†å™¨ç»™ stream_response
    self.stream_response(
      params.answer_stream_port,
      answer_stream_buffer,
      uid,
      workspace_id,
      question.message_id,
      format,
      preferred_ai_model,
      system_prompt,
      agent_config,  // ğŸ”§ ä¼ é€’æ™ºèƒ½ä½“é…ç½®
      tool_call_handler,  // ğŸ”§ ä¼ é€’å·¥å…·è°ƒç”¨å¤„ç†å™¨
    );

    let question_pb = ChatMessagePB::from(question);
    Ok(question_pb)
  }

  #[instrument(level = "info", skip_all, err)]
  pub async fn stream_regenerate_response(
    &self,
    question_id: i64,
    answer_stream_port: i64,
    format: Option<PredefinedFormatPB>,
    ai_model: AIModel,
  ) -> FlowyResult<()> {
    trace!(
      "[Chat] regenerate and stream chat message: chat_id={}",
      self.chat_id,
    );

    // clear
    self
      .stop_stream
      .store(false, std::sync::atomic::Ordering::SeqCst);
    self.stream_buffer.lock().await.clear();

    let format = format.map(Into::into).unwrap_or_default();
    let answer_stream_buffer = self.stream_buffer.clone();
    let uid = self.user_service.user_id()?;
    let workspace_id = self.user_service.workspace_id()?;

    self.stream_response(
      answer_stream_port,
      answer_stream_buffer,
      uid,
      workspace_id,
      question_id,
      format,
      ai_model,
      None, // é‡æ–°ç”Ÿæˆæ—¶ä¸ä½¿ç”¨ç³»ç»Ÿæç¤ºè¯
      None, // ğŸ”§ é‡æ–°ç”Ÿæˆæ—¶ä¸ä½¿ç”¨æ™ºèƒ½ä½“é…ç½®
      None, // ğŸ”§ é‡æ–°ç”Ÿæˆæ—¶ä¸ä½¿ç”¨å·¥å…·è°ƒç”¨å¤„ç†å™¨
    );

    Ok(())
  }

  #[allow(clippy::too_many_arguments)]
  fn stream_response(
    &self,
    answer_stream_port: i64,
    answer_stream_buffer: Arc<Mutex<StringBuffer>>,
    _uid: i64,
    workspace_id: Uuid,
    question_id: i64,
    format: ResponseFormat,
    ai_model: AIModel,
    system_prompt: Option<String>,
    agent_config: Option<AgentConfigPB>,
    tool_call_handler: Option<Arc<crate::agent::ToolCallHandler>>,  // ğŸ”§ æ–°å¢å·¥å…·è°ƒç”¨å¤„ç†å™¨
  ) {
    let stop_stream = self.stop_stream.clone();
    let chat_id = self.chat_id;
    let cloud_service = self.chat_service.clone();
    
    // ğŸ”§ å·¥å…·è°ƒç”¨æ”¯æŒ
    let has_agent = agent_config.is_some();
    let has_tool_handler = tool_call_handler.is_some();
    
    tokio::spawn(async move {
      let mut answer_sink = IsolateSink::new(Isolate::new(answer_stream_port));
      let mut accumulated_text = String::new();  // ğŸ”§ ç´¯ç§¯æ–‡æœ¬ç”¨äºæ£€æµ‹å·¥å…·è°ƒç”¨
      
      // ğŸ”§ å¤šè½®å¯¹è¯æ”¯æŒï¼šè®°å½•å·¥å…·è°ƒç”¨å’Œç»“æœ
      let mut tool_calls_and_results: Vec<(crate::agent::ToolCallRequest, crate::agent::ToolCallResponse)> = Vec::new();
      
      match cloud_service
        .stream_answer_with_system_prompt(&workspace_id, &chat_id, question_id, format.clone(), ai_model.clone(), system_prompt.clone())
        .await
      {
        Ok(mut stream) => {
          while let Some(message) = stream.next().await {
            match message {
              Ok(message) => {
                if stop_stream.load(std::sync::atomic::Ordering::Relaxed) {
                  trace!("[Chat] client stop streaming message");
                  break;
                }
                match message {
                  QuestionStreamValue::Answer { value } => {
                    // ğŸ”§ ç´¯ç§¯æ–‡æœ¬ä»¥æ£€æµ‹å·¥å…·è°ƒç”¨
                    if has_agent {
                      accumulated_text.push_str(&value);
                      
                      // ğŸ› DEBUG: æ¯æ¬¡æ¥æ”¶åˆ°æ•°æ®æ—¶æ‰“å°ç´¯ç§¯æ–‡æœ¬çš„é•¿åº¦
                      // if accumulated_text.len() % 100 == 0 || accumulated_text.len() < 50 {
                      //   info!("ğŸ”§ [DEBUG] Accumulated text length: {} chars", accumulated_text.len());
                      //   if accumulated_text.len() < 200 {
                      //     info!("ğŸ”§ [DEBUG] Current text: {}", accumulated_text);
                      //   } else if accumulated_text.len() <= 300 {
                      //     // å®‰å…¨æˆªå–å‰ 200 å­—ç¬¦
                      //     let mut preview_len = std::cmp::min(200, accumulated_text.len());
                      //     while preview_len > 0 && !accumulated_text.is_char_boundary(preview_len) {
                      //       preview_len -= 1;
                      //     }
                      //     info!("ğŸ”§ [DEBUG] Current text preview: {}", &accumulated_text[..preview_len]);
                      //   }
                      // }
                      
                      // æ£€æµ‹æ˜¯å¦åŒ…å«**å®Œæ•´çš„**å·¥å…·è°ƒç”¨ï¼ˆå¿…é¡»æœ‰å¼€å§‹å’Œç»“æŸæ ‡ç­¾ï¼‰
                      let has_start_tag = accumulated_text.contains("<tool_call>");
                      let has_end_tag = accumulated_text.contains("</tool_call>");
                      
                      // ğŸ”§ åŒæ—¶æ£€æµ‹ markdown ä»£ç å—æ ¼å¼ (AI å¯èƒ½è¯¯ç”¨)
                      let has_markdown_tool_call = accumulated_text.contains("```tool_call") && 
                                                   accumulated_text.contains("```\n");
                      
                      // ğŸ› DEBUG: å¦‚æœæ£€æµ‹åˆ°æ ‡ç­¾,æ‰“å°çŠ¶æ€
                      // if has_start_tag || has_end_tag || has_markdown_tool_call {
                      //   info!("ğŸ”§ [DEBUG] Tool call tags detected - XML start: {}, XML end: {}, Markdown: {}", 
                      //         has_start_tag, has_end_tag, has_markdown_tool_call);
                      // }
                      
                      // å¦‚æœæ£€æµ‹åˆ° markdown æ ¼å¼,è½¬æ¢ä¸º XML æ ¼å¼
                      if has_markdown_tool_call && !has_start_tag {
                        warn!("ğŸ”§ [TOOL] âš ï¸ AI used markdown code block format instead of XML tags! Converting...");
                        accumulated_text = accumulated_text
                          .replace("```tool_call\n", "<tool_call>\n")
                          .replace("\n```", "\n</tool_call>");
                        info!("ğŸ”§ [TOOL] Converted markdown format to XML format");
                      }
                      
                      if has_start_tag && has_end_tag {
                        info!("ğŸ”§ [TOOL] Complete tool call detected in response");
                        
                        // æå–å·¥å…·è°ƒç”¨
                        let calls = crate::agent::ToolCallHandler::extract_tool_calls(&accumulated_text);
                        
                        info!("ğŸ”§ [TOOL] Extracted {} tool calls from accumulated text", calls.len());
                        
                        if calls.is_empty() {
                          warn!("ğŸ”§ [TOOL] âš ï¸ Tool call tag found but extraction failed!");
                          warn!("ğŸ”§ [TOOL] Accumulated text length: {} chars", accumulated_text.len());
                          warn!("ğŸ”§ [TOOL] Number of <tool_call> tags: {}", accumulated_text.matches("<tool_call>").count());
                          warn!("ğŸ”§ [TOOL] Number of </tool_call> tags: {}", accumulated_text.matches("</tool_call>").count());
                          
                          // æ˜¾ç¤ºæ›´é•¿çš„é¢„è§ˆï¼ŒåŒ…æ‹¬å¯èƒ½çš„å¤šä¸ªå·¥å…·è°ƒç”¨
                          let preview_len = std::cmp::min(accumulated_text.len(), 1500);
                          warn!("ğŸ”§ [TOOL] Accumulated text preview (first {} chars):", preview_len);
                          warn!("ğŸ”§ [TOOL] {}", &accumulated_text[..preview_len]);
                        }
                        
                        for (request, start, end) in calls {
                          // å‘é€å·¥å…·è°ƒç”¨å‰çš„æ–‡æœ¬
                          let before_text = &accumulated_text[..start];
                          if !before_text.is_empty() {
                            answer_stream_buffer.lock().await.push_str(before_text);
                            let _ = answer_sink
                              .send(StreamMessage::OnData(before_text.to_string()).to_string())
                              .await;
                          }
                          
                          // å‘é€å·¥å…·è°ƒç”¨å…ƒæ•°æ®ï¼ˆé€šçŸ¥ UI å·¥å…·æ­£åœ¨æ‰§è¡Œï¼‰
                          let tool_metadata = serde_json::json!({
                            "tool_call": {
                              "id": request.id,
                              "tool_name": request.tool_name,
                              "status": "running",
                              "arguments": request.arguments,
                            }
                          });
                          let _ = answer_sink
                            .send(StreamMessage::Metadata(serde_json::to_string(&tool_metadata).unwrap()).to_string())
                            .await;
                          
                          info!("ğŸ”§ [TOOL] Executing tool: {} (id: {})", request.tool_name, request.id);
                          
                          // âœ… å®é™…æ‰§è¡Œå·¥å…·
                          if has_tool_handler {
                            if let Some(ref handler) = tool_call_handler {
                              let response = handler.execute_tool_call(&request, agent_config.as_ref()).await;
                              
                              info!("ğŸ”§ [TOOL] Tool execution completed: {} - success: {}, has_result: {}",
                                    response.id, response.success, response.result.is_some());
                              
                              // ğŸ”§ ä¿å­˜å·¥å…·è°ƒç”¨å’Œç»“æœï¼Œç”¨äºåç»­å¤šè½®å¯¹è¯
                              tool_calls_and_results.push((request.clone(), response.clone()));
                              info!("ğŸ”§ [TOOL] Saved tool result for multi-turn. Total saved: {}", tool_calls_and_results.len());
                              
                              // å‘é€å·¥å…·æ‰§è¡Œç»“æœå…ƒæ•°æ®
                              let result_status = if response.success { "success" } else { "failed" };
                              let result_metadata = serde_json::json!({
                                "tool_call": {
                                  "id": response.id,
                                  "tool_name": request.tool_name,
                                  "status": result_status,
                                  "result": response.result,
                                  "error": response.error,
                                  "duration_ms": response.duration_ms,
                                }
                              });
                              let _ = answer_sink
                                .send(StreamMessage::Metadata(serde_json::to_string(&result_metadata).unwrap()).to_string())
                                .await;
                              
                              // âœ… å°†å·¥å…·æ‰§è¡Œç»“æœå‘é€ç»™ç”¨æˆ·æ˜¾ç¤º
                              // âš ï¸ æ³¨æ„ï¼šè¿™ä¸ªç»“æœç”¨äº UI æ˜¾ç¤ºï¼Œå®é™…çš„å¤šè½®å¯¹è¯é€»è¾‘åœ¨æµç»“æŸåå¤„ç†
                              if response.success {
                                if let Some(result_text) = response.result {
                                  let formatted_result = format!(
                                    "\n<tool_result>\nå·¥å…·æ‰§è¡ŒæˆåŠŸï¼š{}\nç»“æœï¼š{}\n</tool_result>\n",
                                    request.tool_name,
                                    result_text
                                  );
                                  
                                  // å®‰å…¨åœ°ç”Ÿæˆé¢„è§ˆï¼Œé¿å…åœ¨ UTF-8 å­—ç¬¦è¾¹ç•Œä¸­é—´åˆ‡å‰²
                                  let preview = if result_text.len() > 100 {
                                    let mut preview_len = 100.min(result_text.len());
                                    while preview_len > 0 && !result_text.is_char_boundary(preview_len) {
                                      preview_len -= 1;
                                    }
                                    format!("{}...", &result_text[..preview_len])
                                  } else {
                                    result_text.clone()
                                  };
                                  
                                  info!("ğŸ”§ [TOOL] Sending tool result to UI ({}ms): {}", 
                                        response.duration_ms, 
                                        preview);
                                  
                                  // å‘é€å·¥å…·ç»“æœåˆ° UI
                                  answer_stream_buffer.lock().await.push_str(&formatted_result);
                                                let _ = answer_sink
                                    .send(StreamMessage::OnData(formatted_result).to_string())
                                                  .await;
                                  
                                  info!("ğŸ”§ [TOOL] Tool result sent to UI - will be used for follow-up AI response");
                                }
                              } else {
                                // å·¥å…·æ‰§è¡Œå¤±è´¥ï¼Œé€šçŸ¥ç”¨æˆ·
                                let error_msg = format!(
                                  "\n<tool_error>\nå·¥å…·æ‰§è¡Œå¤±è´¥ï¼š{}\né”™è¯¯ï¼š{}\n</tool_error>\n",
                                  request.tool_name,
                                  response.error.unwrap_or_else(|| "Unknown error".to_string())
                                );
                                
                                error!("ğŸ”§ [TOOL] Tool failed: {} - sending error to UI", response.id);
                                
                                answer_stream_buffer.lock().await.push_str(&error_msg);
                                let _ = answer_sink
                                  .send(StreamMessage::OnData(error_msg).to_string())
                                  .await;
                              }
                            }
                          } else {
                            // æ²¡æœ‰å·¥å…·å¤„ç†å™¨ï¼Œå‘é€å ä½æ¶ˆæ¯
                            warn!("ğŸ”§ [TOOL] Tool handler not available, skipping execution");
                            let placeholder_metadata = serde_json::json!({
                              "tool_call": {
                                "id": request.id,
                                "tool_name": request.tool_name,
                                "status": "skipped",
                                "result": "Tool execution not configured",
                              }
                            });
                            let _ = answer_sink
                              .send(StreamMessage::Metadata(serde_json::to_string(&placeholder_metadata).unwrap()).to_string())
                              .await;
                          }
                          
                          // æ¸…é™¤å·²å¤„ç†çš„æ–‡æœ¬
                          accumulated_text = accumulated_text[end..].to_string();
                        }
                        
                        // å‘é€å‰©ä½™æ–‡æœ¬
                        if !accumulated_text.is_empty() {
                          answer_stream_buffer.lock().await.push_str(&accumulated_text);
                          let _ = answer_sink
                            .send(StreamMessage::OnData(accumulated_text.clone()).to_string())
                            .await;
                          accumulated_text.clear();
                        }
                      } else {
                        // æ²¡æœ‰æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ï¼Œæ­£å¸¸å‘é€
                        answer_stream_buffer.lock().await.push_str(&value);
                        if let Err(err) = answer_sink
                          .send(StreamMessage::OnData(value).to_string())
                          .await
                        {
                          error!("Failed to stream answer via IsolateSink: {}", err);
                        }
                      }
                    } else {
                      // æ²¡æœ‰æ™ºèƒ½ä½“é…ç½®ï¼Œæ­£å¸¸å‘é€
                      answer_stream_buffer.lock().await.push_str(&value);
                      if let Err(err) = answer_sink
                        .send(StreamMessage::OnData(value).to_string())
                        .await
                      {
                        error!("Failed to stream answer via IsolateSink: {}", err);
                      }
                    }
                  },
                  QuestionStreamValue::Metadata { value } => {
                    if let Ok(s) = serde_json::to_string(&value) {
                      answer_stream_buffer.lock().await.set_metadata(value);
                      let _ = answer_sink
                        .send(StreamMessage::Metadata(s).to_string())
                        .await;
                    }
                  },
                  QuestionStreamValue::SuggestedQuestion {
                    context_suggested_questions: _,
                  } => {},
                  QuestionStreamValue::FollowUp {
                    should_generate_related_question,
                  } => {
                    let _ = answer_sink
                      .send(
                        StreamMessage::OnFollowUp(AIFollowUpData {
                          should_generate_related_question,
                        })
                        .to_string(),
                      )
                      .await;
                  },
                }
              },
              Err(err) => {
                if err.code == ErrorCode::RequestTimeout || err.code == ErrorCode::Internal {
                  error!("[Chat] unexpected stream error: {}", err);
                  let _ = answer_sink.send(StreamMessage::Done.to_string()).await;
                } else {
                  error!("[Chat] failed to stream answer: {}", err);
                  let _ = answer_sink
                    .send(StreamMessage::OnError(err.msg.clone()).to_string())
                    .await;
                  let pb = ChatMessageErrorPB {
                    chat_id: chat_id.to_string(),
                    error_message: err.to_string(),
                  };
                  chat_notification_builder(chat_id, ChatNotification::StreamChatMessageError)
                    .payload(pb)
                    .send();
                  return Err(err);
                }
              },
            }
          }
          
          // ğŸ”§ åæ€æœºåˆ¶ï¼šå¦‚æœæœ‰å·¥å…·è°ƒç”¨ç»“æœä¸”å¯ç”¨äº†åæ€ï¼Œè¿›å…¥åæ€å¾ªç¯
          info!("ğŸ”§ [REFLECTION] Stream ended - checking for reflection. has_agent: {}, tool_calls_count: {}", 
                has_agent, tool_calls_and_results.len());
          
          if has_agent && !tool_calls_and_results.is_empty() {
            // ğŸ› DEBUG: è¾“å‡ºæ™ºèƒ½ä½“é…ç½®ä¿¡æ¯
            if let Some(ref config) = agent_config {
              info!("ğŸ”§ [REFLECTION] â•â•â• Agent Configuration â•â•â•");
              info!("ğŸ”§ [REFLECTION]   Agent ID: {}", config.id);
              info!("ğŸ”§ [REFLECTION]   Agent Name: {}", config.name);
              info!("ğŸ”§ [REFLECTION]   enable_reflection: {}", config.capabilities.enable_reflection);
              info!("ğŸ”§ [REFLECTION]   max_reflection_iterations: {}", config.capabilities.max_reflection_iterations);
              info!("ğŸ”§ [REFLECTION]   enable_tool_calling: {}", config.capabilities.enable_tool_calling);
              info!("ğŸ”§ [REFLECTION]   max_tool_calls: {}", config.capabilities.max_tool_calls);
              info!("ğŸ”§ [REFLECTION] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
            } else {
              warn!("ğŸ”§ [REFLECTION] âš ï¸ No agent config available!");
            }
            
            // æ£€æŸ¥æ˜¯å¦å¯ç”¨åæ€æœºåˆ¶
            let enable_reflection = agent_config.as_ref()
              .map(|config| config.capabilities.enable_reflection)
              .unwrap_or(false);
            
            let max_iterations = agent_config.as_ref()
              .map(|config| {
                let configured = config.capabilities.max_reflection_iterations;
                if configured <= 0 || !enable_reflection {
                  1 // å¦‚æœæœªå¯ç”¨åæ€æˆ–é…ç½®ä¸º0ï¼Œåˆ™åªæ‰§è¡Œä¸€æ¬¡ï¼ˆä¼ ç»Ÿæ¨¡å¼ï¼‰
                } else {
                  configured.min(10) as usize // æœ€å¤§10æ¬¡è¿­ä»£
                }
              })
              .unwrap_or(1);
            
            info!("ğŸ”§ [REFLECTION] Calculated: enable_reflection={}, max_iterations={}", enable_reflection, max_iterations);
            info!("ğŸ”§ [REFLECTION] Starting reflection loop with {} initial tool call(s)", tool_calls_and_results.len());
            
            // ğŸ”§ åæ€å¾ªç¯ï¼šå¤šæ¬¡è¿­ä»£ç›´åˆ° AI è®¤ä¸ºå¯ä»¥å›ç­”æˆ–è¾¾åˆ°é™åˆ¶
            let mut current_iteration = 0;
            let mut all_tool_results = tool_calls_and_results.clone();
            
            // ä»æ™ºèƒ½ä½“é…ç½®ä¸­è·å–å·¥å…·ç»“æœæœ€å¤§é•¿åº¦é™åˆ¶ï¼Œé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿
            let max_result_length = agent_config.as_ref()
              .map(|config| {
                // ç¡®ä¿å€¼åœ¨åˆç†èŒƒå›´å†…ï¼šæœ€å° 1000ï¼Œé»˜è®¤ 4000
                let configured = config.capabilities.max_tool_result_length;
                if configured <= 0 {
                  4000 // é»˜è®¤å€¼
                } else if configured < 1000 {
                  1000 // æœ€å°å€¼
            } else {
                  configured as usize
                }
              })
              .unwrap_or(4000); // å¦‚æœæ²¡æœ‰é…ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼ 4000
            
            // ğŸ” å¼€å§‹åæ€å¾ªç¯
            while current_iteration < max_iterations {
              current_iteration += 1;
              info!("ğŸ”§ [REFLECTION] â•â•â• Iteration {}/{} â•â•â•", current_iteration, max_iterations);
              info!("ğŸ”§ [REFLECTION] Current tool results count: {}", all_tool_results.len());
              
              // æ„å»ºåŒ…å«æ‰€æœ‰å·¥å…·ç»“æœçš„ä¸Šä¸‹æ–‡æ¶ˆæ¯
              let mut follow_up_context = String::new();
              if current_iteration == 1 {
                follow_up_context.push_str("\n\nä»¥ä¸‹æ˜¯å·¥å…·è°ƒç”¨çš„ç»“æœï¼Œè¯·åŸºäºè¿™äº›ç»“æœå›ç­”ç”¨æˆ·çš„åŸå§‹é—®é¢˜ï¼š\n\n");
              } else {
                follow_up_context.push_str(&format!("\n\nä»¥ä¸‹æ˜¯ç¬¬ {} è½®å·¥å…·è°ƒç”¨çš„æ‰€æœ‰ç»“æœï¼š\n\n", current_iteration));
              }
            
              info!("ğŸ”§ [REFLECTION] Using max_tool_result_length: {} chars", max_result_length);
              
              // éå†æ‰€æœ‰å·¥å…·ç»“æœï¼Œæ„å»ºä¸Šä¸‹æ–‡
              for (idx, (req, resp)) in all_tool_results.iter().enumerate() {
                // ä½¿ç”¨ map å’Œ unwrap_or é¿å…ä¸´æ—¶å€¼ç”Ÿå‘½å‘¨æœŸé—®é¢˜
                let result_text = resp.result.as_ref().map(|s| s.as_str()).unwrap_or("æ— ç»“æœ");
                
                // æ™ºèƒ½æˆªæ–­é•¿ç»“æœ
                let truncated_result = if result_text.len() > max_result_length {
                  // å®‰å…¨æˆªæ–­ï¼Œè€ƒè™‘ UTF-8 å­—ç¬¦è¾¹ç•Œ
                  let mut truncate_len = max_result_length.min(result_text.len());
                  while truncate_len > 0 && !result_text.is_char_boundary(truncate_len) {
                    truncate_len -= 1;
                  }
                  let truncated = &result_text[..truncate_len];
                  info!("ğŸ”§ [REFLECTION] Truncating tool result #{} from {} to {} chars", idx + 1, result_text.len(), truncate_len);
                  format!("{}...\n[ç»“æœå·²æˆªæ–­ï¼ŒåŸå§‹é•¿åº¦: {} å­—ç¬¦]", truncated, result_text.len())
                } else {
                  result_text.to_string()
                };
                
                follow_up_context.push_str(&format!(
                  "å·¥å…·è°ƒç”¨ #{}: {}\nå‚æ•°: {}\nç»“æœ: {}\næ‰§è¡ŒçŠ¶æ€: {}\n\n",
                  idx + 1,
                  req.tool_name,
                  serde_json::to_string_pretty(&req.arguments).unwrap_or_else(|_| "æ— æ³•åºåˆ—åŒ–".to_string()),
                  truncated_result,
                  if resp.success { "æˆåŠŸ" } else { "å¤±è´¥" }
                ));
              }
            
              // æ ¹æ®æ˜¯å¦å¯ç”¨åæ€æœºåˆ¶å’Œå½“å‰è¿­ä»£ï¼Œç»™ AI ä¸åŒçš„æŒ‡ç¤º
              if enable_reflection && current_iteration < max_iterations {
                follow_up_context.push_str(&format!("è¯·è¯„ä¼°è¿™äº›å·¥å…·ç»“æœæ˜¯å¦è¶³ä»¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼ˆå½“å‰ç¬¬ {}/{} è½®ï¼‰ï¼š\n", current_iteration, max_iterations));
                follow_up_context.push_str("- å¦‚æœç»“æœå……åˆ†ï¼Œè¯·ç”¨ä¸­æ–‡ç®€ä½“æ€»ç»“å¹¶ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜\n");
                follow_up_context.push_str("- å¦‚æœç»“æœä¸è¶³æˆ–éœ€è¦æ›´å¤šä¿¡æ¯ï¼Œå¯ä»¥ç»§ç»­è°ƒç”¨å…¶ä»–å¯ç”¨å·¥å…·\n");
                follow_up_context.push_str("- é¿å…è°ƒç”¨å·²ç»å°è¯•è¿‡çš„å·¥å…·æˆ–é‡å¤çš„æŸ¥è¯¢\n");
              } else {
                follow_up_context.push_str("è¯·ç”¨ä¸­æ–‡ç®€ä½“æ€»ç»“å’Œè§£é‡Šè¿™äº›å·¥å…·æ‰§è¡Œç»“æœï¼Œç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œä¸è¦å†æ¬¡è°ƒç”¨å·¥å…·ã€‚\n");
              }
              follow_up_context.push_str("æ³¨æ„ï¼šå¦‚æœç»“æœè¢«æˆªæ–­ï¼Œè¯·åŸºäºå¯ç”¨ä¿¡æ¯ç»™å‡ºæœ€ä½³å›ç­”ã€‚");
            
              // ğŸ› DEBUG: æ‰“å° follow_up_context çš„é¢„è§ˆ
              let context_preview_len = std::cmp::min(500, follow_up_context.len());
              let mut safe_preview_len = context_preview_len;
              while safe_preview_len > 0 && !follow_up_context.is_char_boundary(safe_preview_len) {
                safe_preview_len -= 1;
              }
              info!("ğŸ”§ [REFLECTION] Follow-up context preview: {}...", &follow_up_context[..safe_preview_len]);
              
              // æ„å»ºæ–°çš„ç³»ç»Ÿæç¤ºï¼ˆåŒ…å«åŸæç¤º + å·¥å…·ç»“æœä¸Šä¸‹æ–‡ï¼‰
              let follow_up_system_prompt = if let Some(ref original_prompt) = system_prompt {
                format!("{}\n\n{}", original_prompt, follow_up_context)
              } else {
                follow_up_context
              };
              
              let prompt_len = follow_up_system_prompt.len();
              info!("ğŸ”§ [REFLECTION] Calling AI with follow-up context ({} chars)", prompt_len);
              
              // æ£€æŸ¥ä¸Šä¸‹æ–‡é•¿åº¦
              if prompt_len > 16000 {
                warn!("ğŸ”§ [REFLECTION] âš ï¸ System prompt is very long ({} chars), may exceed model limit", prompt_len);
              }
              
              // å‘é€ä¸€ä¸ªåˆ†éš”ç¬¦ï¼Œè®©ç”¨æˆ·çŸ¥é“ AI æ­£åœ¨ç”Ÿæˆå›ç­”
              if current_iteration == 1 {
                let separator = "\n\n---\n\n";
                answer_stream_buffer.lock().await.push_str(separator);
                let _ = answer_sink
                  .send(StreamMessage::OnData(separator.to_string()).to_string())
                  .await;
              } else {
                let separator = format!("\n\n--- ç¬¬ {}/{} è½®åæ€ ---\n\n", current_iteration, max_iterations);
                answer_stream_buffer.lock().await.push_str(&separator);
                let _ = answer_sink
                  .send(StreamMessage::OnData(separator.clone()).to_string())
                  .await;
              }
            
              // ä½¿ç”¨åŸå§‹é—®é¢˜ + å·¥å…·ç»“æœä¸Šä¸‹æ–‡å†æ¬¡è°ƒç”¨ AI
              info!("ğŸ”§ [REFLECTION] Calling AI with question_id: {}", question_id);
              
              match cloud_service
                .stream_answer_with_system_prompt(
                  &workspace_id, 
                  &chat_id, 
                  question_id, 
                  format.clone(), 
                  ai_model.clone(),
                  Some(follow_up_system_prompt)
                )
                .await
              {
                    Ok(mut follow_up_stream) => {
                      info!("ğŸ”§ [REFLECTION] Follow-up stream started for iteration {}", current_iteration);
                      let mut message_count = 0;
                      let mut answer_chunks = 0;
                      let mut has_received_data = false;
                      let mut reflection_accumulated_text = String::new(); // ğŸ”§ ç´¯ç§¯æ–‡æœ¬ç”¨äºæ£€æµ‹æ–°å·¥å…·è°ƒç”¨
                      let mut new_tool_calls_detected = false;
                      
                      while let Some(message) = follow_up_stream.next().await {
                        message_count += 1;
                        
                        if stop_stream.load(std::sync::atomic::Ordering::Relaxed) {
                          info!("ğŸ”§ [REFLECTION] Stream stopped by user after {} messages", message_count);
                          break;
                        }
                        
                        match message {
                          Ok(message) => {
                            match message {
                              QuestionStreamValue::Answer { value } => {
                                answer_chunks += 1;
                                has_received_data = true;
                                
                                // ğŸ”§ åæ€æœºåˆ¶ï¼šç´¯ç§¯æ–‡æœ¬å¹¶æ£€æµ‹æ–°çš„å·¥å…·è°ƒç”¨
                                if enable_reflection && current_iteration < max_iterations {
                                  reflection_accumulated_text.push_str(&value);
                                  
                                  // æ£€æµ‹æ˜¯å¦åŒ…å«**å®Œæ•´çš„**å·¥å…·è°ƒç”¨
                                  let has_start_tag = reflection_accumulated_text.contains("<tool_call>");
                                  let has_end_tag = reflection_accumulated_text.contains("</tool_call>");
                                  
                                  if has_start_tag && has_end_tag && !new_tool_calls_detected {
                                    info!("ğŸ”§ [REFLECTION] Detected new tool call in iteration {} response!", current_iteration);
                                    new_tool_calls_detected = true;
                                    // ä¸ç«‹å³é€€å‡ºå¾ªç¯ï¼Œç»§ç»­æ¥æ”¶å®Œæ•´çš„å“åº”
                                  }
                                }
                                
                                // å‘é€ç­”æ¡ˆå†…å®¹
                                answer_stream_buffer.lock().await.push_str(&value);
                                let _ = answer_sink
                                  .send(StreamMessage::OnData(value).to_string())
                                  .await;
                              },
                              QuestionStreamValue::Metadata { value } => {
                                if let Ok(s) = serde_json::to_string(&value) {
                                  answer_stream_buffer.lock().await.set_metadata(value);
                                  let _ = answer_sink
                                    .send(StreamMessage::Metadata(s).to_string())
                                    .await;
                                }
                              },
                              _ => {
                                // å¿½ç•¥å…¶ä»–æ¶ˆæ¯ç±»å‹
                              }
                            }
                          },
                          Err(err) => {
                            error!("ğŸ”§ [REFLECTION] Stream error after {} messages: {}", message_count, err);
                            break;
                          }
                        }
                      }
                      
                      info!("ğŸ”§ [REFLECTION] Iteration {} completed: {} messages, {} answer chunks, has_data: {}, new_tools: {}", 
                            current_iteration, message_count, answer_chunks, has_received_data, new_tool_calls_detected);
                      
                      // ğŸ”§ å¤„ç†æ–°æ£€æµ‹åˆ°çš„å·¥å…·è°ƒç”¨
                      if new_tool_calls_detected && has_tool_handler && current_iteration < max_iterations {
                        info!("ğŸ”§ [REFLECTION] Processing new tool calls detected in iteration {}", current_iteration);
                        
                        // æå–æ–°çš„å·¥å…·è°ƒç”¨ï¼ˆè¿”å› Vec<(ToolCallRequest, usize, usize)>ï¼‰
                        let new_calls_raw = crate::agent::ToolCallHandler::extract_tool_calls(&reflection_accumulated_text);
                        let new_calls: Vec<_> = new_calls_raw.into_iter().map(|(req, _, _)| req).collect();
                        info!("ğŸ”§ [REFLECTION] Extracted {} new tool calls", new_calls.len());
                        
                        if !new_calls.is_empty() {
                          // æ‰§è¡Œæ–°çš„å·¥å…·è°ƒç”¨
                          for call in new_calls {
                            info!("ğŸ”§ [REFLECTION] Executing new tool: {} (iteration {})", call.tool_name, current_iteration);
                            
                            if let Some(ref handler) = tool_call_handler {
                              let response = handler.execute_tool_call(&call, agent_config.as_ref()).await;
                              if response.success {
                                info!("ğŸ”§ [REFLECTION] Tool {} executed successfully in iteration {}", call.tool_name, current_iteration);
                              } else {
                                warn!("ğŸ”§ [REFLECTION] Tool {} execution returned success=false in iteration {}", call.tool_name, current_iteration);
                              }
                              all_tool_results.push((call, response));
                            }
                          }
                          
                          // ç»§ç»­ä¸‹ä¸€è½®è¿­ä»£
                          info!("ğŸ”§ [REFLECTION] New tools executed, continuing to iteration {}", current_iteration + 1);
                          continue; // ç»§ç»­ while å¾ªç¯
                        } else {
                          warn!("ğŸ”§ [REFLECTION] Tool call tags found but extraction failed in iteration {}", current_iteration);
                        }
                      }
                      
                      // æ²¡æœ‰æ–°å·¥å…·è°ƒç”¨ï¼Œé€€å‡ºå¾ªç¯
                      info!("ğŸ”§ [REFLECTION] No new tool calls detected, ending reflection loop");
                      
                      // å¦‚æœæ²¡æœ‰æ”¶åˆ°æ•°æ®ï¼Œå‘é€é™çº§æ¶ˆæ¯
                      if !has_received_data {
                        warn!("ğŸ”§ [REFLECTION] âš ï¸ No data received from iteration {} stream!", current_iteration);
                        warn!("ğŸ”§ [REFLECTION]   Possible causes:");
                        warn!("ğŸ”§ [REFLECTION]   1. AI model returned empty response");
                        warn!("ğŸ”§ [REFLECTION]   2. System prompt too long ({} chars)", prompt_len);
                        warn!("ğŸ”§ [REFLECTION]   3. Original question not found for question_id: {}", question_id);
                        warn!("ğŸ”§ [REFLECTION] ğŸ’¡ Fallback: Sending tool result summary to user");
                        
                        // é™çº§æ–¹æ¡ˆï¼šç›´æ¥å‘é€å·¥å…·ç»“æœçš„ç®€å•æ€»ç»“
                        let fallback_message = format!(
                          "\n\nğŸ“Š å·¥å…·æ‰§è¡Œå®Œæˆï¼ˆç¬¬ {}/{} è½®ï¼‰\n\n{} å·¥å…·å·²æˆåŠŸæ‰§è¡Œå¹¶è¿”å›ç»“æœï¼ˆå¦‚ä¸Šæ‰€ç¤ºï¼‰ã€‚\n\nç”±äº AI æœåŠ¡æš‚æ—¶æ— æ³•ç”Ÿæˆè¯¦ç»†æ€»ç»“ï¼Œè¯·æ‚¨ç›´æ¥æŸ¥çœ‹ä¸Šæ–¹çš„å·¥å…·æ‰§è¡Œç»“æœã€‚\n\nğŸ’¡ æç¤ºï¼š\n- å¦‚æœç»“æœè¿‡é•¿ï¼Œè¯·åœ¨æ™ºèƒ½ä½“é…ç½®ä¸­å¢åŠ ã€Œå·¥å…·ç»“æœæœ€å¤§é•¿åº¦ã€\n- æˆ–å°è¯•ä½¿ç”¨æ”¯æŒæ›´é•¿ä¸Šä¸‹æ–‡çš„ AI æ¨¡å‹\n- å½“å‰ System Prompt é•¿åº¦ï¼š{} å­—ç¬¦\n",
                          current_iteration,
                          max_iterations,
                          all_tool_results.len(),
                          prompt_len
                        );
                        
                        answer_stream_buffer.lock().await.push_str(&fallback_message);
                        let _ = answer_sink
                          .send(StreamMessage::OnData(fallback_message).to_string())
                          .await;
                      }
                      
                      break; // é€€å‡º while å¾ªç¯
                    },
                    Err(err) => {
                      error!("ğŸ”§ [REFLECTION] Failed to start stream for iteration {}: {}", current_iteration, err);
                      let error_msg = format!("\n\nç”Ÿæˆå›ç­”æ—¶å‡ºé”™ï¼ˆç¬¬ {}/{} è½®ï¼‰: {}\n", current_iteration, max_iterations, err);
                      answer_stream_buffer.lock().await.push_str(&error_msg);
                      let _ = answer_sink
                        .send(StreamMessage::OnData(error_msg).to_string())
                        .await;
                      break; // é€€å‡º while å¾ªç¯
                    }
              }
            } // end of while loop
            
            info!("ğŸ”§ [REFLECTION] Reflection loop ended after {} iterations with {} total tool results", 
                  current_iteration, all_tool_results.len());
          }
        },
        Err(err) => {
          error!("[Chat] failed to start streaming: {}", err);
          if err.is_ai_response_limit_exceeded() {
            let _ = answer_sink
              .send(StreamMessage::AIResponseLimitExceeded.to_string())
              .await;
          } else if err.is_ai_image_response_limit_exceeded() {
            let _ = answer_sink
              .send(StreamMessage::AIImageResponseLimitExceeded.to_string())
              .await;
          } else if err.is_ai_max_required() {
            let _ = answer_sink
              .send(StreamMessage::AIMaxRequired(err.msg.clone()).to_string())
              .await;
          } else if err.is_local_ai_not_ready() {
            let _ = answer_sink
              .send(StreamMessage::LocalAINotReady(err.msg.clone()).to_string())
              .await;
          } else if err.is_local_ai_disabled() {
            let _ = answer_sink
              .send(StreamMessage::LocalAIDisabled(err.msg.clone()).to_string())
              .await;
          } else {
            let _ = answer_sink
              .send(StreamMessage::OnError(err.msg.clone()).to_string())
              .await;
          }

          let pb = ChatMessageErrorPB {
            chat_id: chat_id.to_string(),
            error_message: err.to_string(),
          };
          chat_notification_builder(chat_id, ChatNotification::StreamChatMessageError)
            .payload(pb)
            .send();
          return Err(err);
        },
      }

      chat_notification_builder(chat_id, ChatNotification::FinishStreaming).send();
      trace!("[Chat] finish streaming");

      if answer_stream_buffer.lock().await.is_empty() {
        return Ok(());
      }
      let content = answer_stream_buffer.lock().await.take_content();
      let metadata = answer_stream_buffer.lock().await.take_metadata();
      let answer = cloud_service
        .create_answer(
          &workspace_id,
          &chat_id,
          content.trim(),
          question_id,
          metadata,
        )
        .await?;
      notify_message(&chat_id, answer)?;
      Ok::<(), FlowyError>(())
    });
  }

  /// Load chat messages for a given `chat_id`.
  ///
  /// 1. When opening a chat:
  ///    - Loads local chat messages.
  ///    - `after_message_id` and `before_message_id` are `None`.
  ///    - Spawns a task to load messages from the remote server, notifying the user when the remote messages are loaded.
  ///
  /// 2. Loading more messages in an existing chat with `after_message_id`:
  ///    - `after_message_id` is the last message ID in the current chat messages.
  ///
  /// 3. Loading more messages in an existing chat with `before_message_id`:
  ///    - `before_message_id` is the first message ID in the current chat messages.
  pub async fn load_prev_chat_messages(
    &self,
    limit: u64,
    before_message_id: Option<i64>,
  ) -> Result<ChatMessageListPB, FlowyError> {
    trace!(
      "[Chat] Loading messages from disk: chat_id={}, limit={}, before_message_id={:?}",
      self.chat_id, limit, before_message_id
    );

    let offset = before_message_id.map_or(MessageCursor::NextBack, MessageCursor::BeforeMessageId);
    let messages = self.load_local_chat_messages(limit, offset).await?;

    // If the number of messages equals the limit, then no need to load more messages from remote
    if messages.len() == limit as usize {
      let pb = ChatMessageListPB {
        messages,
        has_more: true,
        total: 0,
      };
      chat_notification_builder(self.chat_id, ChatNotification::DidLoadPrevChatMessage)
        .payload(pb.clone())
        .send();
      return Ok(pb);
    }

    if matches!(
      *self.prev_message_state.read().await,
      PrevMessageState::HasMore
    ) {
      *self.prev_message_state.write().await = PrevMessageState::Loading;
      if let Err(err) = self
        .load_remote_chat_messages(limit, before_message_id, None)
        .await
      {
        error!("Failed to load previous chat messages: {}", err);
      }
    }

    Ok(ChatMessageListPB {
      messages,
      has_more: true,
      total: 0,
    })
  }

  pub async fn load_latest_chat_messages(
    &self,
    limit: u64,
    after_message_id: Option<i64>,
  ) -> Result<ChatMessageListPB, FlowyError> {
    trace!(
      "[Chat] Loading new messages: chat_id={}, limit={}, after_message_id={:?}",
      self.chat_id, limit, after_message_id,
    );
    let offset = after_message_id.map_or(MessageCursor::NextBack, MessageCursor::AfterMessageId);
    let messages = self.load_local_chat_messages(limit, offset).await?;

    trace!(
      "[Chat] Loaded local chat messages: chat_id={}, messages={}",
      self.chat_id,
      messages.len()
    );

    // If the number of messages equals the limit, then no need to load more messages from remote
    let has_more = !messages.is_empty();
    let _ = self
      .load_remote_chat_messages(limit, None, after_message_id)
      .await;
    Ok(ChatMessageListPB {
      messages,
      has_more,
      total: 0,
    })
  }

  async fn load_remote_chat_messages(
    &self,
    limit: u64,
    before_message_id: Option<i64>,
    after_message_id: Option<i64>,
  ) -> FlowyResult<()> {
    trace!(
      "[Chat] start loading messages from remote: chat_id={}, limit={}, before_message_id={:?}, after_message_id={:?}",
      self.chat_id, limit, before_message_id, after_message_id
    );
    let workspace_id = self.user_service.workspace_id()?;
    let chat_id = self.chat_id;
    let cloud_service = self.chat_service.clone();
    let user_service = self.user_service.clone();
    let uid = self.uid;
    let prev_message_state = self.prev_message_state.clone();
    let latest_message_id = self.latest_message_id.clone();
    tokio::spawn(async move {
      let cursor = match (before_message_id, after_message_id) {
        (Some(bid), _) => MessageCursor::BeforeMessageId(bid),
        (_, Some(aid)) => MessageCursor::AfterMessageId(aid),
        _ => MessageCursor::NextBack,
      };
      match cloud_service
        .get_chat_messages(&workspace_id, &chat_id, cursor.clone(), limit)
        .await
      {
        Ok(resp) => {
          // Save chat messages to local disk
          if let Err(err) = save_chat_message_disk(
            user_service.sqlite_connection(uid)?,
            &chat_id,
            resp.messages.clone(),
            true,
          ) {
            error!("Failed to save chat:{} messages: {}", chat_id, err);
          }

          // Update latest message ID
          if !resp.messages.is_empty() {
            latest_message_id.store(
              resp.messages[0].message_id,
              std::sync::atomic::Ordering::Relaxed,
            );
          }

          let pb = ChatMessageListPB::from(resp);
          trace!(
            "[Chat] Loaded messages from remote: chat_id={}, messages={}, hasMore: {}, cursor:{:?}",
            chat_id,
            pb.messages.len(),
            pb.has_more,
            cursor,
          );
          if matches!(cursor, MessageCursor::BeforeMessageId(_)) {
            if pb.has_more {
              *prev_message_state.write().await = PrevMessageState::HasMore;
            } else {
              *prev_message_state.write().await = PrevMessageState::NoMore;
            }
            chat_notification_builder(chat_id, ChatNotification::DidLoadPrevChatMessage)
              .payload(pb)
              .send();
          } else {
            chat_notification_builder(chat_id, ChatNotification::DidLoadLatestChatMessage)
              .payload(pb)
              .send();
          }
        },
        Err(err) => error!("Failed to load chat messages: {}", err),
      }
      Ok::<(), FlowyError>(())
    });
    Ok(())
  }

  pub async fn get_question_id_from_answer_id(
    &self,
    chat_id: &Uuid,
    answer_message_id: i64,
  ) -> Result<i64, FlowyError> {
    let conn = self.user_service.sqlite_connection(self.uid)?;

    let local_result =
      select_answer_where_match_reply_message_id(conn, &chat_id.to_string(), answer_message_id)?
        .map(|message| message.message_id);

    if let Some(message_id) = local_result {
      return Ok(message_id);
    }

    let workspace_id = self.user_service.workspace_id()?;
    let chat_id = self.chat_id;
    let cloud_service = self.chat_service.clone();

    let question = cloud_service
      .get_question_from_answer_id(&workspace_id, &chat_id, answer_message_id)
      .await?;

    Ok(question.message_id)
  }

  pub async fn get_related_question(
    &self,
    message_id: i64,
    ai_model: AIModel,
  ) -> Result<RepeatedRelatedQuestionPB, FlowyError> {
    let workspace_id = self.user_service.workspace_id()?;
    let resp = self
      .chat_service
      .get_related_message(&workspace_id, &self.chat_id, message_id, ai_model)
      .await?;

    trace!(
      "[Chat] related messages: chat_id={}, message_id={}, messages:{:?}",
      self.chat_id, message_id, resp.items
    );
    Ok(RepeatedRelatedQuestionPB::from(resp))
  }

  #[instrument(level = "debug", skip_all, err)]
  pub async fn generate_answer(&self, question_message_id: i64) -> FlowyResult<ChatMessagePB> {
    trace!(
      "[Chat] generate answer: chat_id={}, question_message_id={}",
      self.chat_id, question_message_id
    );
    let workspace_id = self.user_service.workspace_id()?;
    let answer = self
      .chat_service
      .get_answer(&workspace_id, &self.chat_id, question_message_id)
      .await?;

    notify_message(&self.chat_id, answer.clone())?;
    let pb = ChatMessagePB::from(answer);
    Ok(pb)
  }

  async fn load_local_chat_messages(
    &self,
    limit: u64,
    offset: MessageCursor,
  ) -> Result<Vec<ChatMessagePB>, FlowyError> {
    trace!(
      "[Chat] Loading messages from disk: chat_id={}, limit={}, offset={:?}",
      self.chat_id, limit, offset
    );
    let conn = self.user_service.sqlite_connection(self.uid)?;
    let rows = select_chat_messages(conn, &self.chat_id.to_string(), limit, offset)?.messages;
    let messages = rows
      .into_iter()
      .map(|record| ChatMessagePB {
        message_id: record.message_id,
        content: record.content,
        created_at: record.created_at,
        author_type: record.author_type,
        author_id: record.author_id,
        reply_message_id: record.reply_message_id,
        metadata: record.metadata,
      })
      .collect::<Vec<_>>();

    Ok(messages)
  }

  #[instrument(level = "debug", skip_all, err)]
  pub async fn index_file(&self, file_path: PathBuf) -> FlowyResult<()> {
    if !file_path.exists() {
      return Err(
        FlowyError::record_not_found().with_context(format!("{:?} not exist", file_path)),
      );
    }

    if !file_path.is_file() {
      return Err(
        FlowyError::invalid_data().with_context(format!("{:?} is not a file ", file_path)),
      );
    }

    trace!(
      "[Chat] index file: chat_id={}, file_path={:?}",
      self.chat_id, file_path
    );
    self
      .chat_service
      .embed_file(
        &self.user_service.workspace_id()?,
        &file_path,
        &self.chat_id,
        None,
      )
      .await?;

    trace!(
      "[Chat] created index file record: chat_id={}, file_path={:?}",
      self.chat_id, file_path
    );

    Ok(())
  }
}

fn save_chat_message_disk(
  conn: DBConnection,
  chat_id: &Uuid,
  messages: Vec<ChatMessage>,
  is_sync: bool,
) -> FlowyResult<()> {
  let records = messages
    .into_iter()
    .map(|message| ChatMessageTable {
      message_id: message.message_id,
      chat_id: chat_id.to_string(),
      content: message.content,
      created_at: message.created_at.timestamp(),
      author_type: message.author.author_type as i64,
      author_id: message.author.author_id.to_string(),
      reply_message_id: message.reply_message_id,
      metadata: Some(serde_json::to_string(&message.metadata).unwrap_or_default()),
      is_sync,
    })
    .collect::<Vec<_>>();
  upsert_chat_messages(conn, &records)?;
  Ok(())
}

#[derive(Debug, Default)]
struct StringBuffer {
  content: String,
  metadata: Option<serde_json::Value>,
}

impl StringBuffer {
  fn clear(&mut self) {
    self.content.clear();
    self.metadata = None;
  }

  fn push_str(&mut self, value: &str) {
    self.content.push_str(value);
  }

  fn set_metadata(&mut self, value: serde_json::Value) {
    self.metadata = Some(value);
  }

  fn is_empty(&self) -> bool {
    self.content.is_empty()
  }

  fn take_metadata(&mut self) -> Option<serde_json::Value> {
    self.metadata.take()
  }

  fn take_content(&mut self) -> String {
    std::mem::take(&mut self.content)
  }
}

pub(crate) fn notify_message(chat_id: &Uuid, message: ChatMessage) -> Result<(), FlowyError> {
  trace!("[Chat] save answer: answer={:?}", message);
  let pb = ChatMessagePB::from(message);
  chat_notification_builder(chat_id, ChatNotification::DidReceiveChatMessage)
    .payload(pb)
    .send();

  Ok(())
}
